{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-11T12:48:46.720427Z",
     "iopub.status.busy": "2025-08-11T12:48:46.719707Z",
     "iopub.status.idle": "2025-08-11T12:48:48.604437Z",
     "shell.execute_reply": "2025-08-11T12:48:48.603606Z",
     "shell.execute_reply.started": "2025-08-11T12:48:46.720399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bge-reranker-finetune-3/config.json\n",
      "/kaggle/input/bge-reranker-finetune-3/zero_to_fp32.py\n",
      "/kaggle/input/bge-reranker-finetune-3/trainer_state.json\n",
      "/kaggle/input/bge-reranker-finetune-3/training_args.bin\n",
      "/kaggle/input/bge-reranker-finetune-3/tokenizer.json\n",
      "/kaggle/input/bge-reranker-finetune-3/tokenizer_config.json\n",
      "/kaggle/input/bge-reranker-finetune-3/model.safetensors\n",
      "/kaggle/input/bge-reranker-finetune-3/special_tokens_map.json\n",
      "/kaggle/input/bge-reranker-finetune-3/latest\n",
      "/kaggle/input/bge-reranker-finetune-3/rng_state.pth\n",
      "/kaggle/input/bge-reranker-finetune-3/sentencepiece.bpe.model\n",
      "/kaggle/input/output-with-teacherscore-minedhn/output_with_teacherscore_minedHN.jsonl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T12:48:48.605868Z",
     "iopub.status.busy": "2025-08-11T12:48:48.605378Z",
     "iopub.status.idle": "2025-08-11T12:48:49.465704Z",
     "shell.execute_reply": "2025-08-11T12:48:49.464872Z",
     "shell.execute_reply.started": "2025-08-11T12:48:48.605841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "full_train_data = []\n",
    "prompt = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "with open(\"/kaggle/input/output-with-teacherscore-minedhn/output_with_teacherscore_minedHN.jsonl\", \"r\", encoding = \"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            temp_dict = json.loads(line)\n",
    "            temp_dict['prompt'] = prompt\n",
    "            full_train_data.append(temp_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T12:48:49.467745Z",
     "iopub.status.busy": "2025-08-11T12:48:49.467512Z",
     "iopub.status.idle": "2025-08-11T12:48:49.866895Z",
     "shell.execute_reply": "2025-08-11T12:48:49.866203Z",
     "shell.execute_reply.started": "2025-08-11T12:48:49.467728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save to a regular JSON file (list of dicts)\n",
    "with open(\"train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_train_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-08-11T12:48:49.867914Z",
     "iopub.status.busy": "2025-08-11T12:48:49.867666Z",
     "iopub.status.idle": "2025-08-11T12:51:29.907406Z",
     "shell.execute_reply": "2025-08-11T12:51:29.906638Z",
     "shell.execute_reply.started": "2025-08-11T12:48:49.867889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m816.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U FlagEmbedding[finetune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T12:51:29.908676Z",
     "iopub.status.busy": "2025-08-11T12:51:29.908429Z",
     "iopub.status.idle": "2025-08-11T12:51:42.680720Z",
     "shell.execute_reply": "2025-08-11T12:51:42.679686Z",
     "shell.execute_reply.started": "2025-08-11T12:51:29.908644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y -q flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T12:51:42.682164Z",
     "iopub.status.busy": "2025-08-11T12:51:42.681917Z",
     "iopub.status.idle": "2025-08-11T12:52:11.109662Z",
     "shell.execute_reply": "2025-08-11T12:52:11.109075Z",
     "shell.execute_reply.started": "2025-08-11T12:51:42.682139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf7976c3cd041189b7b2c0e2a09f866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc8a9c07c244d2388917e7aa510680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503f119005cb4a16bdbc902c5d585d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff27efee5514d6db2a14839a10eaa7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "def truncate_passages(passages, max_length=512):\n",
    "    \"\"\"Truncate each passage in the list to max_length tokens.\"\"\"\n",
    "    truncated = []\n",
    "    for passage in passages:\n",
    "        tokens = tokenizer(\n",
    "            passage,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None  # We only want token ids\n",
    "        )\n",
    "        truncated_passage = tokenizer.decode(tokens[\"input_ids\"], skip_special_tokens=True)\n",
    "        truncated.append(truncated_passage)\n",
    "    return truncated\n",
    "\n",
    "# Truncate all pos/neg passages in full_train_data\n",
    "for item in full_train_data:\n",
    "    item[\"pos\"] = truncate_passages(item.get(\"pos\", []))\n",
    "    item[\"neg\"] = truncate_passages(item.get(\"neg\", []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T12:52:11.110748Z",
     "iopub.status.busy": "2025-08-11T12:52:11.110329Z",
     "iopub.status.idle": "2025-08-11T12:52:11.294480Z",
     "shell.execute_reply": "2025-08-11T12:52:11.293758Z",
     "shell.execute_reply.started": "2025-08-11T12:52:11.110728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to /kaggle/working/full_train_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/kaggle/working/full_train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Saved to /kaggle/working/full_train_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T12:52:11.295559Z",
     "iopub.status.busy": "2025-08-11T12:52:11.295209Z",
     "iopub.status.idle": "2025-08-11T12:52:11.302080Z",
     "shell.execute_reply": "2025-08-11T12:52:11.301461Z",
     "shell.execute_reply.started": "2025-08-11T12:52:11.295539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds_stage0 = {\n",
    "    \"zero_optimization\": {\n",
    "      \"stage\": 1\n",
    "    },\n",
    "    \"fp16\": {\n",
    "      \"enabled\": False\n",
    "    },\n",
    "    \"bf16\": {\n",
    "      \"enabled\": False\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupDecayLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\",\n",
    "            \"total_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 100,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": False,\n",
    "}\n",
    "\n",
    "with open(\"ds_stage0.json\", \"w\") as f:\n",
    "    json.dump(ds_stage0, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T12:52:11.304337Z",
     "iopub.status.busy": "2025-08-11T12:52:11.304154Z",
     "iopub.status.idle": "2025-08-11T12:52:20.215432Z",
     "shell.execute_reply": "2025-08-11T12:52:20.214813Z",
     "shell.execute_reply.started": "2025-08-11T12:52:11.304323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import wandb\n",
    "wandb.login(key=\"yourapikey\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T14:38:36.203875Z",
     "iopub.status.busy": "2025-08-11T14:38:36.202975Z",
     "iopub.status.idle": "2025-08-11T15:22:34.151421Z",
     "shell.execute_reply": "2025-08-11T15:22:34.150572Z",
     "shell.execute_reply.started": "2025-08-11T14:38:36.203839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=1 \\\n",
    "  -m FlagEmbedding.finetune.reranker.encoder_only.base \\\n",
    "  --model_name_or_path /kaggle/working/test_encoder_only_base_bge-reranker-v2-m3/checkpoint-2190 \\\n",
    "  --cache_dir $HOME/.cache/huggingface/hub \\\n",
    "  --train_data /kaggle/working/full_train_data.json \\\n",
    "  --cache_path ~/.cache \\\n",
    "  --train_group_size 4 \\\n",
    "  --query_max_len 128 \\\n",
    "  --passage_max_len 512 \\\n",
    "  --pad_to_multiple_of 8 \\\n",
    "  --knowledge_distillation False \\\n",
    "  --output_dir ./test_encoder_only_base_bge-reranker-v2-m3 \\\n",
    "  --overwrite_output_dir \\\n",
    "  --learning_rate 6e-5 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --dataloader_drop_last True \\\n",
    "  --warmup_ratio 0.1 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --weight_decay 0.01 \\\n",
    "  --deepspeed /kaggle/working/ds_stage0.json \\\n",
    "  --logging_steps 1 \\\n",
    "  --save_steps 10000"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7975527,
     "sourceId": 12623068,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8048082,
     "sourceId": 12732583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
