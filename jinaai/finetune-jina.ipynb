{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12623068,"sourceType":"datasetVersion","datasetId":7975527}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:29.410366Z","iopub.execute_input":"2025-08-05T13:52:29.410647Z","iopub.status.idle":"2025-08-05T13:52:29.420229Z","shell.execute_reply.started":"2025-08-05T13:52:29.410626Z","shell.execute_reply":"2025-08-05T13:52:29.419490Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/output-with-teacherscore-minedhn/output_with_teacherscore_minedHN.jsonl\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers.evaluation import InformationRetrievalEvaluator\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:29.421493Z","iopub.execute_input":"2025-08-05T13:52:29.422328Z","iopub.status.idle":"2025-08-05T13:52:29.428846Z","shell.execute_reply.started":"2025-08-05T13:52:29.422303Z","shell.execute_reply":"2025-08-05T13:52:29.428120Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n# model.to(\"cuda\")\n\n# Dual encoder access\nquery_encoder = model[0]\npassage_encoder = model[1]\n\n# Set task heads explicitly\nquery_encoder.default_task = \"retrieval.query\"\npassage_encoder.default_task = \"retrieval.passage\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:29.429456Z","iopub.execute_input":"2025-08-05T13:52:29.429699Z","iopub.status.idle":"2025-08-05T13:52:32.772257Z","shell.execute_reply.started":"2025-08-05T13:52:29.429683Z","shell.execute_reply":"2025-08-05T13:52:32.771482Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import json\nfull_train_data = []\nprompt = \"Represent this sentence for searching relevant passages: \"\n\nwith open(\"/kaggle/input/output-with-teacherscore-minedhn/output_with_teacherscore_minedHN.jsonl\", \"r\", encoding = \"utf-8\") as f:\n    for line in f:\n        line = line.strip()\n        if line:\n            temp_dict = json.loads(line)\n            temp_dict['prompt'] = prompt\n            full_train_data.append(temp_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:32.774142Z","iopub.execute_input":"2025-08-05T13:52:32.774370Z","iopub.status.idle":"2025-08-05T13:52:33.114253Z","shell.execute_reply.started":"2025-08-05T13:52:32.774354Z","shell.execute_reply":"2025-08-05T13:52:33.113515Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Full dataset\nall_data = full_train_data  # list of your dicts\n\n# Split into train and temp (eval+test)\ntrain_data, temp_data = train_test_split(all_data, test_size=0.2, random_state=42)\neval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\n# train_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:33.115065Z","iopub.execute_input":"2025-08-05T13:52:33.115312Z","iopub.status.idle":"2025-08-05T13:52:33.125239Z","shell.execute_reply.started":"2025-08-05T13:52:33.115287Z","shell.execute_reply":"2025-08-05T13:52:33.124696Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch.nn as nn\n\nclass CustomDualEncoder(nn.Module):\n    def __init__(self, query_encoder, passage_encoder):\n        super().__init__()\n        self.query_encoder = query_encoder\n        self.passage_encoder = passage_encoder\n\n    def forward(self, features):\n        # features is a single dictionary containing \"texts\" with 2 texts per sample\n        # SentenceTransformer will automatically tokenize and split per encoder\n        query_features = {k: v for k, v in features.items() if k.endswith(\"0\")}\n        passage_features = {k: v for k, v in features.items() if k.endswith(\"1\")}\n\n        # Clean keys: remove the 0/1 suffix to make them usable\n        query_features = {k[:-1]: v for k, v in query_features.items()}\n        passage_features = {k[:-1]: v for k, v in passage_features.items()}\n\n        query_embedding = self.query_encoder(query_features)[\"sentence_embedding\"]\n        passage_embedding = self.passage_encoder(passage_features)[\"sentence_embedding\"]\n\n        return [query_embedding, passage_embedding]\n\n\ndual_model = CustomDualEncoder(query_encoder, passage_encoder)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:33.125972Z","iopub.execute_input":"2025-08-05T13:52:33.126153Z","iopub.status.idle":"2025-08-05T13:52:33.140146Z","shell.execute_reply.started":"2025-08-05T13:52:33.126133Z","shell.execute_reply":"2025-08-05T13:52:33.139644Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Create training examples (positive query-passage pairs)\ndef prepare_multiple_negatives_data(data_dicts, prompt=\"\"):\n    examples = []\n    for d in data_dicts:\n        query = prompt + d['query']\n        for pos_passage in d['pos']:\n            examples.append(InputExample(texts=[query, pos_passage]))\n    return examples\n\ntrain_examples = prepare_multiple_negatives_data(train_data, prompt=\"Represent this sentence for searching relevant passages: \")\n\nprint(train_examples[0])\n# Wrap in a DataLoader\n\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\n# Use MultipleNegativesRankingLoss\ntrain_loss = losses.MultipleNegativesRankingLoss(model = dual_model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:33.140725Z","iopub.execute_input":"2025-08-05T13:52:33.140889Z","iopub.status.idle":"2025-08-05T13:52:33.159781Z","shell.execute_reply.started":"2025-08-05T13:52:33.140878Z","shell.execute_reply":"2025-08-05T13:52:33.159184Z"}},"outputs":[{"name":"stdout","text":"<InputExample> label: 0, texts: Represent this sentence for searching relevant passages: Cơ quan nào có thẩm quyền cấp lại giấy chứng nhận đăng ký doanh nghiệp?; 1. Cơ quan đăng ký kinh doanh được tổ chức ở tỉnh, thành phố trực thuộc Trung ương (sau đây gọi chung là cấp tỉnh) và ở quận, huyện, thị xã, thành phố thuộc tỉnh (sau đây gọi chung là cấp huyện), bao gồm:a) Ở cấp tỉnh: Phòng Đăng ký kinh doanh thuộc Sở Kế hoạch và Đầu tư (sau đây gọi chung là Phòng Đăng ký kinh doanh).Phòng Đăng ký kinh doanh có thể tổ chức các điểm để tiếp nhận hồ sơ và trả kết quả thuộc Phòng Đăng ký kinh doanh tại các địa điểm khác nhau trên địa bàn cấp tỉnh;b) Ở cấp huyện: Phòng Tài chính - Kế hoạch thuộc Ủy ban nhân dân cấp huyện (sau đây gọi chung là Cơ quan đăng ký kinh doanh cấp huyện).\n\n2. Cơ quan đăng ký kinh doanh có tài khoản và con dấu riêng.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"def prepare_evaluator_input(data, prompt=\"\"):\n    queries = {}\n    corpus = {}\n    relevant_docs = {}\n    corpus_id_counter = 0\n\n    for idx, entry in enumerate(data):\n        qid = f\"q{idx}\"\n        queries[qid] = prompt + entry[\"query\"]\n        relevant_docs[qid] = []\n\n        for pos in entry[\"pos\"]:\n            pid = f\"p{corpus_id_counter}\"\n            corpus[pid] = pos\n            relevant_docs[qid].append(pid)\n            corpus_id_counter += 1\n\n        for neg in entry.get(\"neg\", []):\n            pid = f\"p{corpus_id_counter}\"\n            corpus[pid] = neg\n            corpus_id_counter += 1\n\n    return queries, corpus, relevant_docs\n\n# prepare evaluator \neval_queries, eval_corpus, eval_relevant_docs = prepare_evaluator_input(eval_data, prompt=\"Represent this sentence for searching relevant passages: \")\ntest_queries, test_corpus, test_relevant_docs = prepare_evaluator_input(test_data, prompt=\"Represent this sentence for searching relevant passages: \")\n\nevaluator = InformationRetrievalEvaluator(\n    queries=eval_queries,\n    corpus=eval_corpus,\n    relevant_docs=eval_relevant_docs,\n    name=\"eval\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:33.160626Z","iopub.execute_input":"2025-08-05T13:52:33.160880Z","iopub.status.idle":"2025-08-05T13:52:33.175979Z","shell.execute_reply.started":"2025-08-05T13:52:33.160860Z","shell.execute_reply":"2025-08-05T13:52:33.175306Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nimport wandb\nwandb.login(key=\"yourapikey\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:33.176678Z","iopub.execute_input":"2025-08-05T13:52:33.176876Z","iopub.status.idle":"2025-08-05T13:52:33.188946Z","shell.execute_reply.started":"2025-08-05T13:52:33.176862Z","shell.execute_reply":"2025-08-05T13:52:33.188250Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# Train\nprint(\"Starting training...\")\nfrom sentence_transformers import SentenceTransformer\n\nnew_model = SentenceTransformer(modules=[])\n\nnew_model.model = dual_model\n\n# Now train using model.fit()\nprint(\"Starting training...\")\nnew_model.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    evaluator=evaluator,\n    evaluation_steps=10,\n    epochs=3,\n    warmup_steps=100,\n    show_progress_bar=True,\n    use_amp=True\n)\nprint(\"Training complete.\")\n\n# Save the fine-tuned model\nnew_model.save(\"output/jina-v3-asym-finetune-1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:54:27.943314Z","iopub.execute_input":"2025-08-05T13:54:27.943873Z","iopub.status.idle":"2025-08-05T13:54:28.179280Z","shell.execute_reply.started":"2025-08-05T13:54:27.943851Z","shell.execute_reply":"2025-08-05T13:54:28.178411Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nStarting training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4209861962.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Now train using model.fit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m new_model.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_objectives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/fit_mixin.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOriginalCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         trainer = SentenceTransformerTrainer(\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, train_dataset, eval_dataset, loss, evaluator, data_collator, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_card_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedTokenizerBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n","\u001b[0;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'tokenizer'"],"ename":"AttributeError","evalue":"'SentenceTransformer' object has no attribute 'tokenizer'","output_type":"error"}],"execution_count":44},{"cell_type":"code","source":"test_evaluator = InformationRetrievalEvaluator(\n    queries=test_queries,\n    corpus=test_corpus,\n    relevant_docs=test_relevant_docs,\n    name=\"test\",\n    score_function=\"cos_sim\"\n)\n\ntest_evaluator(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:52:33.219888Z","iopub.status.idle":"2025-08-05T13:52:33.220164Z","shell.execute_reply.started":"2025-08-05T13:52:33.219994Z","shell.execute_reply":"2025-08-05T13:52:33.220007Z"}},"outputs":[],"execution_count":null}]}