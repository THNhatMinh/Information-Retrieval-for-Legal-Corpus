{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12688857,"sourceType":"datasetVersion","datasetId":8018640},{"sourceId":12688869,"sourceType":"datasetVersion","datasetId":8018645},{"sourceId":12701412,"sourceType":"datasetVersion","datasetId":8027170}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Proper way to load standard JSON\nwith open(\"/kaggle/input/legal-corpus/legal_corpus.json\", \"r\", encoding=\"utf-8\") as f:\n    corpus_data = json.load(f)  # âœ… returns list of dicts\n\nwith open(\"/kaggle/input/public-test-for-corpus/public_test.json\", \"r\", encoding=\"utf-8\") as f:\n    query_data = json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncorpus_articles = []\nfor doc in corpus_data:\n    for article in doc[\"content\"]:\n        corpus_articles.append({\n            \"aid\": article[\"aid\"],\n            \"content\": article[\"content_Article\"]\n        })\n\n# Prepare text lists\nquestions = [item[\"question\"] for item in query_data]\ndocuments = [item[\"content\"] for item in corpus_articles]\n\nmax_chars = 1024\ndocuments = [doc[:max_chars] for doc in documents]\n\n\n# Encode questions\nquery_embeddings = model.encode(\n    questions,\n    task=\"retrieval.query\",\n    prompt_name=\"retrieval.query\",\n    convert_to_numpy=True\n)\n\n# Encode corpus documents\ncorpus_embeddings = model.encode(\n    documents,\n    task=\"retrieval.passage\",\n    prompt_name=\"retrieval.passage\",\n    convert_to_numpy=True,\n    batch_size=16,  # Try 32 or even 16 if it still crashes\n    show_progress_bar=True\n)\n\n# Optional: Map embeddings to aid for retrieval\naid_to_embedding = {\n    article[\"aid\"]: emb for article, emb in zip(corpus_articles, corpus_embeddings)\n}\n\n# Print shapes\nprint(\"Query embeddings shape:\", query_embeddings.shape)\nprint(\"Corpus embeddings shape:\", corpus_embeddings.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q faiss-cpu rank_bm25\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport faiss\nimport json\nfrom rank_bm25 import BM25Okapi\n\n# ------------ 1. Prepare Your Data ------------\n# Your corpus_articles = list of {\"aid\": ..., \"content_Article\": ...}\n# Your query_data = list of {\"qid\": ..., \"question\": ...}\n# Your corpus_embeddings and query_embeddings = numpy arrays (float32, shape: [n_docs, dim])\n\n# Step 1: BM25\ntokenized_corpus = [doc['content'].split() for doc in corpus_articles]\nbm25 = BM25Okapi(tokenized_corpus)\n\n# Step 2: FAISS\ncorpus_embeddings = np.array(corpus_embeddings).astype(\"float32\")\nquery_embeddings = np.array(query_embeddings).astype(\"float32\")\n\nfaiss.normalize_L2(corpus_embeddings)\nfaiss.normalize_L2(query_embeddings)\n\ndimension = corpus_embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)\nindex.add(corpus_embeddings)\n\n# ------------ 2. Hybrid Search ------------\ntop_k = 50\nalpha = 0.5  # Weight for BM25 vs embedding\n\ntop_k_results = []\n\nfor i, query in enumerate(query_data):\n    tokenized_query = query[\"question\"].split()\n\n    # BM25 scores\n    bm25_scores = np.array(bm25.get_scores(tokenized_query))\n\n    # Embedding similarity scores\n    D, I = index.search(query_embeddings[i].reshape(1, -1), len(corpus_articles))\n    embedding_scores = D[0]  # [len(corpus)]\n\n    # Normalize both scores\n    bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-8)\n    embed_norm = (embedding_scores - embedding_scores.min()) / (embedding_scores.max() - embedding_scores.min() + 1e-8)\n\n    # Combine them\n    hybrid_scores = alpha * bm25_norm + (1 - alpha) * embed_norm\n\n    # Get top-k documents\n    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n    top_aids = [corpus_articles[j][\"aid\"] for j in top_indices]\n\n    top_k_results.append({\n        \"qid\": query[\"id\"],\n        \"relevant_laws\": top_aids\n    })\n\n# ------------ 3. Save to JSON ------------\nwith open(\"hybrid_results.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(top_k_results, f, ensure_ascii=False, indent=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -q -U FlagEmbedding\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from FlagEmbedding import FlagReranker\nmodel = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nimport wandb\nwandb.login(key=\"d49a0c6fc5d5a70d03d24d2934d735b98e170a90\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    with torch.no_grad():\n        scores = model.compute_score(pairs, max_length=1024, doc_type=\"text\")\n\n    # Sort by descending score\n    reranked = sorted(zip(aids, scores), key=lambda x: x[1], reverse=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Strip to only include qid and relevant_laws\nsimplified_results = [\n    {\"qid\": item[\"id\"], \"relevant_laws\": item[\"relevant_laws\"]}\n    for item in reranked_results\n]\n\n# Save to JSON file\nwith open(\"reranked_results.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(simplified_results, f, ensure_ascii=False, indent=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}